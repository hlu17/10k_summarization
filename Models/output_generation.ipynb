{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from rouge import Rouge\n",
    "from bert_score import BERTScorer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import pipeline, set_seed, GPT2Tokenizer\n",
    "from torchsummary import summary\n",
    "from transformers import BartTokenizer,BartModel, BartConfig, BartForConditionalGeneration\n",
    "from transformers.modeling_outputs import Seq2SeqLMOutput\n",
    "import bert_score\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import warnings\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the .pkl file into a pandas DataFrame\n",
    "file_path = 'item7_text_v5.pkl'\n",
    "df = pd.read_pickle(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_interest_expense(row):\n",
    "    \"\"\"\n",
    "    Combines the values of 'Interest expense' and 'Interest Expense' keys, and removes the 'Not Important' key if present.\n",
    "    Args:\n",
    "        row (dict): A dictionary containing the keys to be processed.\n",
    "    Returns:\n",
    "        dict: The updated dictionary with combined interest expense values and without the 'Not Important' key.\n",
    "    \"\"\"\n",
    "    # Combine the values of the two interest expense keys\n",
    "    combined_expense = row['Interest expense'] + row['Interest Expense']\n",
    "    \n",
    "    # Update the 'Interest Expense' key with the combined value and remove the 'Interest expense' key\n",
    "    row['Interest Expense'] = str(combined_expense)\n",
    "    del row['Interest expense']\n",
    "    # Remove the 'Not Important' key, if present\n",
    "    if 'Not Important' in row:\n",
    "        del row['Not Important']\n",
    "    \n",
    "    return row\n",
    "\n",
    "def combine_business_overview(row):\n",
    "    \"\"\"\n",
    "    Combines the values of 'Business Overview' and 'Business overview' keys.\n",
    "    Args:\n",
    "        row (dict): A dictionary containing the keys to be processed.\n",
    "    Returns:\n",
    "        dict: The updated dictionary with combined business overview values.\n",
    "    \"\"\"\n",
    "    # Combine the values of the two business overview keys\n",
    "    combined_business = row['Business Overview'] + row['Business overview']\n",
    "    \n",
    "    # Update the 'Business Overview' key with the combined value and remove the 'Business overview' key\n",
    "    row['Business Overview'] = str(combined_business)\n",
    "    del row['Business overview']\n",
    "    \n",
    "    return row\n",
    "\n",
    "def combine_operations(row):\n",
    "    \"\"\"\n",
    "    Combines the values of 'Results of Operations' and 'Results of operations' keys.\n",
    "    Args:\n",
    "        row (dict): A dictionary containing the keys to be processed.\n",
    "    Returns:\n",
    "        dict: The updated dictionary with combined results of operations values.\n",
    "    \"\"\"\n",
    "    # Combine the values of the two results of operations keys\n",
    "    combined_operation = row['Results of Operations'] + row['Results of operations']\n",
    "    \n",
    "    # Update the 'Results of Operations' key with the combined value and remove the 'Results of operations' key\n",
    "    row['Results of Operations'] = str(combined_operation)\n",
    "    del row['Results of operations']\n",
    "    \n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the custom functions to the DataFrame\n",
    "filtered_df = df.copy()\n",
    "filtered_df['report'] = filtered_df['report'].apply(combine_interest_expense)\n",
    "filtered_df['report'] = filtered_df['report'].apply(combine_business_overview)\n",
    "filtered_df['report'] = filtered_df['report'].apply(combine_operations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count non-blank values for each key\n",
    "non_blank_counts = {}\n",
    "for index, row in filtered_df.iterrows():\n",
    "    report = row['report']\n",
    "    for key, value in report.items():\n",
    "        if value.strip() != \"\":\n",
    "            non_blank_counts[key] = non_blank_counts.get(key, 0) + 1\n",
    "\n",
    "print(\"Counts of non-blank values for each key:\")\n",
    "for key, count in non_blank_counts.items():\n",
    "    print(f\"{key}: {count}\")\n",
    "\n",
    "# Create report output\n",
    "filtered_df['report_output'] = \"\"\n",
    "for index, row in filtered_df.iterrows():\n",
    "    report_output = \"\"\n",
    "    output = row['Output']\n",
    "    for key, value in output.items():\n",
    "        if value != \"\":\n",
    "            report_output += key + \": \" + value + \"\\n\\n\"\n",
    "    filtered_df.at[index, 'report_output'] = report_output\n",
    "\n",
    "filtered_df['output_length'] = filtered_df['report_output'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Calculate Rouge and BERTScores\n",
    "rouge = Rouge()\n",
    "scorer = BERTScorer(lang=\"en\")\n",
    "\n",
    "rouge1_scores = []\n",
    "rouge2_scores = []\n",
    "rougeL_scores = []\n",
    "bert_precision = []\n",
    "bert_recall = []\n",
    "bert_f1 = []\n",
    "\n",
    "for idx, row in filtered_df.iterrows():\n",
    "    label = row['label']\n",
    "    report_output = row['report_output']\n",
    "\n",
    "    rouge_scores = rouge.get_scores(report_output, label, avg=True)\n",
    "    rouge1_scores.append(rouge_scores['rouge-1']['f'])\n",
    "    rouge2_scores.append(rouge_scores['rouge-2']['f'])\n",
    "    rougeL_scores.append(rouge_scores['rouge-l']['f'])\n",
    "\n",
    "    P, R, F1 = scorer.score([report_output], [label])\n",
    "    bert_precision.append(P.item())\n",
    "    bert_recall.append(R.item())\n",
    "    bert_f1.append(F1.item())\n",
    "\n",
    "filtered_df['rouge1_final'] = rouge1_scores\n",
    "filtered_df['rouge2_final'] = rouge2_scores\n",
    "filtered_df['rougeL_final'] = rougeL_scores\n",
    "filtered_df['bertscore_f1_final'] = bert_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove underscore sentences\n",
    "def remove_underscore_sentences(text):\n",
    "    sentences = text.split('. ')\n",
    "    cleaned_sentences = [sentence for sentence in sentences if not re.search(r'_+', sentence)]\n",
    "    return '. '.join(cleaned_sentences)\n",
    "\n",
    "\n",
    "keys = [\n",
    "    'Business Overview',\n",
    "    'Results of Operations',\n",
    "    'Revenues',\n",
    "    'Gross Profit Margin',\n",
    "    'Operating Income',\n",
    "    'Interest Expense',\n",
    "    'Liquidity',\n",
    "    'Debt'\n",
    "]\n",
    "\n",
    "# Clean the 'Output' column by removing underscore sentences\n",
    "for index, row in filtered_df.iterrows():\n",
    "    report = row['Output']\n",
    "    for key in keys:\n",
    "        if key in report:\n",
    "            report[key] = remove_underscore_sentences(report[key])\n",
    "\n",
    "# Overwrite the 'Output' column with the cleaned data\n",
    "filtered_df['Output'] = filtered_df['Output'].apply(lambda report: {key: remove_underscore_sentences(report[key]) for key in keys if key in report})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a folder named \"outputs\"\n",
    "if not os.path.exists(\"outputs\"):\n",
    "    os.makedirs(\"outputs\")\n",
    "\n",
    "# loop through each row in the dataframe\n",
    "for index, row in new_filtered_df.iterrows():\n",
    "    # create the filename for the text file\n",
    "    filename = f\"outputs/{row['id']}_final_model.txt\"\n",
    "    # open the file in write mode\n",
    "    with open(filename, \"w\") as f:\n",
    "        # write the report_output string to the file\n",
    "        f.write(row['report_output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_model(model_checkpoint_name):\n",
    "    # Load BART model\n",
    "    bart_tokenizer = AutoTokenizer.from_pretrained(model_checkpoint_name)\n",
    "    bart_model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint_name)\n",
    "    return bart_tokenizer,bart_model\n",
    "\n",
    "# Load the pretrained BART model and tokenizer\n",
    "tokenizer, model = initial_model(\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input(tokenizer, my_text):\n",
    "    input_ids = tokenizer(\n",
    "        my_text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=1024  # Increased max_length to 2048\n",
    "    )\n",
    "    return input_ids\n",
    "\n",
    "def generate_summary(input_ids, model):\n",
    "    generated_tokens = model.generate(\n",
    "        input_ids=input_ids['input_ids'],\n",
    "        attention_mask=input_ids['attention_mask'],\n",
    "        max_length=1024,  # Increased max_length to 2048\n",
    "        no_repeat_ngram_size=2,\n",
    "        num_beams=64\n",
    "    )\n",
    "    summary = tokenizer.decode(\n",
    "        generated_tokens[0],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    return summary\n",
    "\n",
    "\n",
    "def split_text(text, lines_per_chunk=25):\n",
    "    lines = text.split('. ')\n",
    "    chunks = ['. '.join(lines[i:i + lines_per_chunk]) for i in range(0, len(lines), lines_per_chunk)]\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def process_report_bartbaseline(report):\n",
    "\n",
    "    output = {}\n",
    "    selected_sections = [\n",
    "        'Business Overview',\n",
    "        'Results of Operations',\n",
    "        'Revenues',\n",
    "        'Gross Profit Margin',\n",
    "        'Operating Income',\n",
    "        'Interest Expense',\n",
    "        'Liquidity',\n",
    "        'Debt'\n",
    "    ]\n",
    "\n",
    "    for key in selected_sections:\n",
    "        content = report.get(key, '')\n",
    "        if content:  # Check if content is not empty\n",
    "            chunks = split_text(content)\n",
    "            summarized_chunks = []\n",
    "            for chunk in chunks:\n",
    "                input_ids = process_input(tokenizer, chunk)\n",
    "                summarized_chunk = generate_summary(input_ids, model)\n",
    "                summarized_chunks.append(summarized_chunk)\n",
    "            output[key] = ' '.join(summarized_chunks)\n",
    "        else:\n",
    "            output[key] = ''  # Set output as empty if content is empty\n",
    "    \n",
    "    filtered_output = {key: output[key] for key in selected_sections}\n",
    "    print(filtered_output)\n",
    "    return filtered_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply the function to each row in the 'report' column and save the result in a new column 'bart_output'\n",
    "new_filtered_df['output_baseline'] = new_filtered_df['report'].apply(process_report_bartbaseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_filtered_df['output_baseline_report'] = \"\"\n",
    "\n",
    "for index, row in new_filtered_df.iterrows():\n",
    "    report_output = \"\"\n",
    "    output = row['output_baseline']\n",
    "    \n",
    "    for key, value in output.items():\n",
    "        if value != \"\":\n",
    "            report_output += key + \": \" + value + \"\\n\\n\"\n",
    "            \n",
    "    new_filtered_df.at[index, 'output_baseline_report'] = report_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_filtered_df['baseline_output_length'] = new_filtered_df['output_baseline_report'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming you have a DataFrame called new_filtered_df with columns 'label' and 'report_output'\n",
    "rouge = Rouge()\n",
    "scorer = BERTScorer(lang=\"en\")\n",
    "\n",
    "rouge1_scores = []\n",
    "rouge2_scores = []\n",
    "rougeL_scores = []\n",
    "bert_f1 = []\n",
    "\n",
    "for idx, row in new_filtered_df.iterrows():\n",
    "    label = row['label']\n",
    "    report_output = row['output_baseline_report']\n",
    "\n",
    "    # Calculate Rouge scores\n",
    "    rouge_scores = rouge.get_scores(report_output, label, avg=True)\n",
    "    rouge1_scores.append(rouge_scores['rouge-1']['f'])\n",
    "    rouge2_scores.append(rouge_scores['rouge-2']['f'])\n",
    "    rougeL_scores.append(rouge_scores['rouge-l']['f'])\n",
    "\n",
    "    # Calculate BERTScores\n",
    "    P, R, F1 = scorer.score([report_output], [label])\n",
    "    bert_precision.append(P.item())\n",
    "    bert_recall.append(R.item())\n",
    "    bert_f1.append(F1.item())\n",
    "\n",
    "# Add the scores to the DataFrame\n",
    "new_filtered_df['rouge1_baseline'] = rouge1_scores\n",
    "new_filtered_df['rouge2_baseline'] = rouge2_scores\n",
    "new_filtered_df['rougeL_baseline'] = rougeL_scores\n",
    "new_filtered_df['bertscore_f1_baseline'] = bert_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create a folder named \"outputs\"\n",
    "if not os.path.exists(\"outputs\"):\n",
    "    os.makedirs(\"outputs\")\n",
    "\n",
    "# loop through each row in the dataframe\n",
    "for index, row in new_filtered_df.iterrows():\n",
    "    # create the filename for the text file\n",
    "    filename = f\"outputs/{row['id']}_baseline_model.txt\"\n",
    "    # open the file in write mode\n",
    "    with open(filename, \"w\") as f:\n",
    "        # write the report_output string to the file\n",
    "        f.write(row['output_baseline_report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_scores = new_filtered_df[['rouge1_baseline', 'rouge2_baseline', 'rougeL_baseline', 'bertscore_f1_baseline']].mean()\n",
    "\n",
    "print(\"Mean Scores:\")\n",
    "print(\"ROUGE-1: \", mean_scores['rouge1_baseline'])\n",
    "print(\"ROUGE-2: \", mean_scores['rouge2_baseline'])\n",
    "print(\"ROUGE-L: \", mean_scores['rougeL_baseline'])\n",
    "print(\"BERTScore F1: \", mean_scores['bertscore_f1_baseline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
